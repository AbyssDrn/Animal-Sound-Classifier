# ğŸ¾ **Low-Power Animal Audio Classifier for Biodiversity Monitoring**

*A TinyML Project for Real-Time Species Detection on Edge Devices*

This repository contains a complete end-to-end workflow for building a **deep learning model** that classifies animals based on their **audio calls**, and deploying it on **low-power microcontrollers** such as the Raspberry Pi Pico using **TensorFlow Lite Micro**.

The entire workflowâ€”from preprocessing, training, evaluation, and TFLite quantizationâ€”was developed by **Amal Madhu**.

---

# ğŸ“Œ **Table of Contents**

* [Project Overview](#project-overview)
* [Supported Species](#supported-species)
* [Machine Learning Pipeline](#machine-learning-pipeline)
* [Model Architecture](#model-architecture)
* [Model Performance](#model-performance)
* [TinyML Deployment (Raspberry Pi Pico)](#tinyml-deployment-raspberry-pi-pico)
* [Repository Structure](#repository-structure)
* [How to Run the Notebook](#how-to-run-the-notebook)
* [How to Deploy on Raspberry Pi Pico](#how-to-deploy-on-raspberry-pi-pico)
* [Example MicroPython Inference Script](#example-micropython-inference-script)
* [Future Enhancements](#future-enhancements)
* [Author](#author)

---

# ğŸ¦ **Project Overview**

This project implements a **2D Convolutional Neural Network (CNN)** trained on **Mel Spectrograms** of animal audio recordings.

It is designed for:

âœ” Offline wildlife monitoring
âœ” Low-power embedded systems
âœ” Nature conservation projects
âœ” Real-time species identification

Training is performed in **Google Colab** using TensorFlow/Keras.
The final model is converted to a **quantized TensorFlow Lite model** suitable for TinyML deployment.

---

# ğŸ¾ **Supported Species**

The classifier currently recognizes **10 species**:

| ID | Species |
| -- | ------- |
| 1  | Bird    |
| 2  | Cat     |
| 3  | Chicken |
| 4  | Cow     |
| 5  | Dog     |
| 6  | Donkey  |
| 7  | Frog    |
| 8  | Lion    |
| 9  | Monkey  |
| 10 | Sheep   |

You can expand this by adding your own audio dataset.

---

# ğŸ§  **Machine Learning Pipeline**

The file **SiMoni.ipynb** contains the entire workflow:

### **1ï¸âƒ£ Environment Setup**

Installs required libraries such as:

* TensorFlow / Keras
* Librosa
* Scikit-learn
* Matplotlib
* NumPy

---

### **2ï¸âƒ£ Data Loading**

Dataset expected structure:

```
dataset/
   â”œâ”€â”€ Bird/
   â”œâ”€â”€ Cat/
   â”œâ”€â”€ Chicken/
   â”œâ”€â”€ â€¦
```

Each folder contains `.wav` or `.mp3` audio files.

---

### **3ï¸âƒ£ Preprocessing & Feature Extraction**

âœ” Audio loaded at **22,050 Hz**
âœ” Trimmed/padded to **3 seconds**
âœ” Converted into **Mel Spectrograms** (2D image-like input)

This is the modelâ€™s primary input.

---

### **4ï¸âƒ£ CNN Architecture Overview**

A typical configuration:

* Conv2D â†’ BatchNorm â†’ ReLU
* MaxPooling2D
* Conv2D â†’ BatchNorm â†’ ReLU
* Dropout
* Flatten
* Dense (Softmax for classification)

---

### **5ï¸âƒ£ Training**

* Achieves up to **~97.7% validation accuracy**
* Includes accuracy/loss learning curves

---

### **6ï¸âƒ£ Evaluation**

You get:

* Accuracy plot
* Loss plot
* Confusion matrix
* Classification report (precision, recall, f1-score)

---

### **7ï¸âƒ£ TensorFlow Lite Quantization**

Model is converted using:

```
tf.float16 quantization
```

This results in:

âœ” Smaller file size
âœ” Suitable for Raspberry Pi Pico / ESP32 / Arduino Nano BLE Sense
âœ” Faster inference

---

# ğŸ§© **Model Performance**

Your notebook automatically generates:

* **Confusion Matrix**
* **Accuracy & Loss Graphs**
* **Precision/Recall/F1 Report**

These files are saved in the repo after training.

---

# ğŸ **TinyML Deployment (Raspberry Pi Pico)**

> **Important:** This project uses **audio**, not camera vision.
> You will need a **microphone module**, not a camera.

### âœ” Recommended Hardware

| Component                      | Purpose                     |
| ------------------------------ | --------------------------- |
| **Raspberry Pi Pico / Pico W** | Main MCU                    |
| **I2S Microphone** (INMP441)   | Best for high-quality audio |
| **PDM Microphone** (MSM261D)   | Ultra-low-power option      |
| **OLED Display (Optional)**    | Show detected species       |
| **SD Card Module (Optional)**  | Store audio / model files   |

---

# âš™ï¸ **Deployment Workflow**

### **1. Flash MicroPython to Pico**

Download UF2 â†’ drag into Pico storage.

---

### **2. Install TensorFlow Lite Micro**

Use `tflm` MicroPython library or custom build.

---

### **3. Upload Deployment Files**

Upload these generated files via Thonny:

```
animal_classifier_quantized.tflite
model_labels.txt
species_database.json
main.py
```

---

### **4. Run inference on live microphone audio**

MicroPython script provided below.

---

# ğŸ§ª **Example MicroPython Inference Script**

```python
import tflite_micro as tflm
import audio_processor
import time

# Load labels
with open('model_labels.txt', 'r') as f:
    labels = [line.strip() for line in f]

# Load model
model_data = open('animal_classifier_quantized.tflite', 'rb').read()
interpreter = tflm.runtime.Interpreter(model_data)

print("--- Starting Animal Audio Monitor ---")

while True:
    # Record 3 seconds of audio
    audio_buffer = audio_processor.record_audio(duration=3, sample_rate=22050)

    # Convert audio to Mel Spectrogram
    spectrogram = audio_processor.compute_mel_spectrogram(audio_buffer)

    # Set input tensor
    interpreter.set_input(spectrogram, 0)

    # Run inference
    interpreter.invoke()

    # Get output probabilities
    output = interpreter.get_output(0)
    predicted_index = output.argmax()
    confidence = output[predicted_index]
    predicted_species = labels[predicted_index]

    if confidence > 0.75:
        print(f"Detected: {predicted_species} ({confidence*100:.1f}% confidence)")

    time.sleep(1)
```

---

# ğŸ“ **Repository Structure**

```
ğŸ“¦ animal-audio-classifier
â”‚
â”œâ”€â”€ SiMoni.ipynb                   # Main Colab notebook
â”œâ”€â”€ saved_files.zip
|     â””â”€â”€ animal_classifier_full.keras
|     â””â”€â”€ animal_classifier_quantizedtflite
|     â””â”€â”€ best_model.keras
|     â””â”€â”€ label_classes.npy
|     â””â”€â”€ model_metadata.json
|     â””â”€â”€ species_database.json
|     â””â”€â”€ trainig_history.csv
â”‚
â”œâ”€â”€ species_data/
â”‚     â””â”€â”€ images/                  # Downloaded species images
â”‚
â”‚
â””â”€â”€ README.md                      # Project documentation
```

---

# ğŸ–¥ï¸ **How to Run the Notebook**

### **1. Clone the repository**

```bash
git clone [https://github.com/your-username/animal-audio-classifier.git](https://github.com/AbyssDrn/Animal-Sound-Classifier.git)
cd animal-audio-classifier
```

### **2. Install dependencies**

```bash
pip install -r requirements.txt
```

*(Create `requirements.txt` from your Colab pip installs.)*

### **3. Open in Google Colab**

* Upload **SiMoni.ipynb**
* Upload your audio dataset
* Update `DATA_PATH`
* Run all cells

### **4. Export the files**

Download:

* `.tflite`
* `.json`
* `.txt`

---

# ğŸ”® **Future Enhancements**

âœ” Add more species
âœ” Implement real-time streaming processing
âœ” Build a mobile app interface
âœ” Support environmental sound classification
âœ” Add GPS tagging + SD card storage
âœ” Build a solar-powered edge device

---

# ğŸ‘¤ **Author**

**Amal Madhu**
Developer â€¢ AI & TinyML Researcher
GitHub: *AbyssDrn*

---

